<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fiche Technique : DeepSeek-TNG-R1T2-Chimera</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 40px auto;
            padding: 20px;
            background-color: #f4f7f6;
        }
        .card {
            background: #fff;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            border-left: 6px solid #2c3e50;
        }
        h1 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.8rem;
        }
        .badge {
            display: inline-block;
            background: #e67e22;
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: bold;
            margin-bottom: 15px;
        }
        .specs-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 25px;
            border-top: 1px solid #eee;
            padding-top: 20px;
        }
        .spec-item {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }
        .spec-value {
            display: block;
            font-size: 1.4rem;
            font-weight: bold;
            color: #2980b9;
        }
        .spec-label {
            font-size: 0.9rem;
            color: #7f8c8d;
            text-transform: uppercase;
        }
        .description {
            margin-top: 20px;
            font-style: italic;
            color: #555;
        }
    </style>
</head>
<body>

<div class="card">
    <div class="badge">TNG Tech - 2nd Generation</div>
    <h1>DeepSeek-TNG-R1T2-Chimera</h1>
    
    <p class="description">
        Un modèle Mixture-of-Experts (MoE) de 671 milliards de paramètres, fusionnant les checkpoints R1-0528, R1 et V3-0324 via une méthode "Assembly-of-Experts".
    </p>

    <div class="specs-grid">
        <div class="spec-item">
            <span class="spec-value">671B</span>
            <span class="spec-label">Paramètres</span>
        </div>
        <div class="spec-item">
            <span class="spec-value">+20%</span>
            <span class="spec-label">Vitesse vs R1</span>
        </div>
        <div class="spec-item">
            <span class="spec-value">60k - 130k</span>
            <span class="spec-label">Fenêtre Contexte</span>
        </div>
        <div class="spec-item">
            <span class="spec-value">2x Faster</span>
            <span class="spec-label">vs R1-0528 (vLLM)</span>
        </div>
    </div>

    <div style="margin-top: 25px;">
        <h3>Points Clés :</h3>
        <ul>
            <li>Performance de raisonnement accrue (Tri-parent design).</li>
            <li>Comportement des tokens &lt;think&gt; constant.</li>
            <li>Optimisé pour l'analyse de long contexte et le dialogue ouvert.</li>
        </ul>
    </div>
</div>

</body>
</html>
